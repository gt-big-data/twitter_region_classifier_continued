{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 1.08 s, total: 2.42 s\n",
      "Wall time: 5.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = MongoClient('mongodb://143.215.138.132:27017')['big_data']\n",
    "\n",
    "matchNE = {'$match': {'lat': {'$gte': 36, '$lte': 50}, 'lon': {'$gte': -99, '$lte': -69}}}\n",
    "matchSE = {'$match': {'lat': {'$gte': 25, '$lte': 36}, 'lon': {'$gte': -99, '$lte': -69}}}\n",
    "matchNW = {'$match': {'lat': {'$gte': 36, '$lte': 50}, 'lon': {'$gte': -125, '$lte': -99}}}\n",
    "matchSW = {'$match': {'lat': {'$gte': 25, '$lte': 36}, 'lon': {'$gte': -125, '$lte': -99}}}\n",
    "\n",
    "sentence_list = []\n",
    "location_list = []\n",
    "\n",
    "limit = {'$limit': 10000}\n",
    "\n",
    "pipeline = [matchNE, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('NE')\n",
    "\n",
    "pipeline = [matchSE, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('SE')\n",
    "\n",
    "pipeline = [matchNW, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('NW')\n",
    "\n",
    "pipeline = [matchSW, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('SW')\n",
    "    \n",
    "# Add your own sentence here\n",
    "your_sentence = \"It's snowing in New York!\"\n",
    "sentence_list.append(your_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet_Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = TweetTokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return self.wnl.tokenize(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible extra features\n",
    "1. Use hashtags and mentions instead of the whole sentence\n",
    "2. Try Bi-gram or tri-gram\n",
    "3. Measure the formality of language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import dok_matrix, hstack\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "def make_features(corpus):\n",
    "#     vectorizer = CountVectorizer(tokenizer=Tweet_Tokenizer(), analyzer='word', min_df=0)\n",
    "#     return vectorizer.fit_transform(corpus), vectorizer.get_feature_names()\n",
    "\n",
    "    # Basic BOW\n",
    "    vectorizer = CountVectorizer(tokenizer=Tweet_Tokenizer(), analyzer='word', min_df=2)\n",
    "    X_BOW = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Additional Features (need to add more)\n",
    "    NUM_OF_FEATS = 1\n",
    "    X_ADD = dok_matrix((len(corpus), NUM_OF_FEATS))\n",
    "    tt = Tweet_Tokenizer()\n",
    "\n",
    "    # Find Length Percentage\n",
    "    num_token_list = np.array([len(tt.__call__(text)) for text in corpus])\n",
    "    num_token_list = np.argsort(num_token_list)\n",
    "    length_percentage_dict = {num_token_list[i]: i * 1.0 / len(num_token_list) for i in range(len(num_token_list))}\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        X_ADD[i, 0] = length_percentage_dict[i]\n",
    "\n",
    "    # Concatenate\n",
    "    X = hstack([X_BOW, X_ADD])\n",
    "\n",
    "    return X, vectorizer.get_feature_names() + ['<length>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.coo.coo_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "CPU times: user 4.62 s, sys: 35.4 ms, total: 4.65 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentence_vector_list, vector_name_list = make_features(sentence_list)\n",
    "\n",
    "print(type(sentence_vector_list))\n",
    "\n",
    "# coo_matrix doesn't directly support slicing, so change to csr_matrix first\n",
    "sentence_vector_list = sentence_vector_list.tocsr()\n",
    "\n",
    "print(type(sentence_vector_list))\n",
    "\n",
    "your_sentence_vector = sentence_vector_list[-1]\n",
    "sentence_vector_list = sentence_vector_list[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_vectors, test_vectors, training_locations, test_locations =\\\n",
    "    train_test_split(sentence_vector_list, location_list, test_size=0.1, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 33 ms, total: 20.9 s\n",
      "Wall time: 20.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=5000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_clf.fit(training_vectors, training_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 ms, sys: 1.73 ms, total: 3.38 ms\n",
      "Wall time: 3.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_test_locations = lr_clf.predict(test_vectors)\n",
    "predicted_your_sentence_location = lr_clf.predict(your_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.407\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy: \"\\\n",
    "      + str(accuracy_score(test_locations, predicted_test_locations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of your sentence: ['NE']\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of your sentence: \" + str(predicted_your_sentence_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Find Top Weights in Logistic Regression Classifier\n",
    "theta_NE = lr_clf.coef_[0]\n",
    "theta_NW = lr_clf.coef_[1]\n",
    "theta_SE = lr_clf.coef_[2]\n",
    "theta_SW = lr_clf.coef_[3]\n",
    "weights_NE = dict()\n",
    "weights_NW = dict()\n",
    "weights_SE = dict()\n",
    "weights_SW = dict()\n",
    "\n",
    "for feature, weight in zip(vector_name_list, theta_NE):\n",
    "    weights_NE[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_NW):\n",
    "    weights_NW[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_SE):\n",
    "    weights_SE[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_SW):\n",
    "    weights_SW[feature] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Scoring Logistic Regression Weights\n",
      "\n",
      "#nyc\t\tNE\t\t1.85779817614\n",
      "#specialreport\t\tNE\t\t1.85539005115\n",
      "ebay\t\tNE\t\t1.76209599626\n",
      "ny\t\tNE\t\t1.74850212026\n",
      "ohio\t\tNE\t\t1.72863709174\n",
      "@bottomoso\t\tNE\t\t1.70721632819\n",
      "#ictfl16\t\tNE\t\t1.69661452756\n",
      "nj\t\tNE\t\t1.68112004575\n",
      "toronto\t\tNE\t\t1.66831803083\n",
      "@steff__r\t\tNE\t\t1.62920485587\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "@cloacamaxima01\t\tNW\t\t2.31485372902\n",
      "portland\t\tNW\t\t2.0079116999\n",
      "@russia\t\tNW\t\t1.89330674312\n",
      "seattle\t\tNW\t\t1.85715166493\n",
      "utah\t\tNW\t\t1.71289058186\n",
      "colorado\t\tNW\t\t1.69212637619\n",
      "nv\t\tNW\t\t1.68236195248\n",
      "bay\t\tNW\t\t1.66897650202\n",
      "francisco\t\tNW\t\t1.66698843905\n",
      "stanford\t\tNW\t\t1.65136205396\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "@gibson326\t\tSE\t\t2.24946795864\n",
      "fl\t\tSE\t\t2.21889621671\n",
      "@taylorswift13\t\tSE\t\t2.11677551163\n",
      "alabama\t\tSE\t\t2.10656323013\n",
      "@negrosubversive\t\tSE\t\t1.91848620291\n",
      "@mycfe\t\tSE\t\t1.76701420979\n",
      "orleans\t\tSE\t\t1.74811972502\n",
      "#ghc16\t\tSE\t\t1.74038570243\n",
      "tx\t\tSE\t\t1.6961710089\n",
      "butter\t\tSE\t\t1.62572447488\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "az\t\tSW\t\t2.13486938964\n",
      "@im_beyondgreat\t\tSW\t\t2.11078860978\n",
      "ca\t\tSW\t\t2.10358254254\n",
      "#losangeles\t\tSW\t\t2.00118848696\n",
      "disneyland\t\tSW\t\t1.95807636057\n",
      "hollywood\t\tSW\t\t1.95055820239\n",
      "@oxthebeardog\t\tSW\t\t1.84792543614\n",
      "arizona\t\tSW\t\t1.8059349652\n",
      "#la\t\tSW\t\t1.76640510335\n",
      "steroline\t\tSW\t\t1.75921907214\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Scoring Logistic Regression Weights\\n\")\n",
    "top_features_NE = sorted(weights_NE, key=lambda x:weights_NE[x], reverse=True)[:10]\n",
    "for word in top_features_NE:\n",
    "    print(str(word) + '\\t\\tNE\\t\\t' + str(weights_NE[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_NW = sorted(weights_NW, key=lambda x:weights_NW[x], reverse=True)[:10]\n",
    "for word in top_features_NW:\n",
    "    print(str(word) + '\\t\\tNW\\t\\t' + str(weights_NW[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_SE = sorted(weights_SE, key=lambda x:weights_SE[x], reverse=True)[:10]\n",
    "for word in top_features_SE:\n",
    "    print(str(word) + '\\t\\tSE\\t\\t' + str(weights_SE[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_SW = sorted(weights_SW, key=lambda x:weights_SW[x], reverse=True)[:10]\n",
    "for word in top_features_SW:\n",
    "    print(str(word) + '\\t\\tSW\\t\\t' + str(weights_SW[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find more in Chapter 1 and 2 of the book below\n",
    "https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
