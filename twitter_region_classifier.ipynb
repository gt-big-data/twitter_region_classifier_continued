{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 962 ms, total: 2.18 s\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "db = MongoClient('mongodb://143.215.138.132:27017')['big_data']\n",
    "\n",
    "matchNE = {'$match': {'lat': {'$gte': 36, '$lte': 50}, 'lon': {'$gte': -99, '$lte': -69}}}\n",
    "matchSE = {'$match': {'lat': {'$gte': 25, '$lte': 36}, 'lon': {'$gte': -99, '$lte': -69}}}\n",
    "matchNW = {'$match': {'lat': {'$gte': 36, '$lte': 50}, 'lon': {'$gte': -125, '$lte': -99}}}\n",
    "matchSW = {'$match': {'lat': {'$gte': 25, '$lte': 36}, 'lon': {'$gte': -125, '$lte': -99}}}\n",
    "\n",
    "sentence_list = []\n",
    "location_list = []\n",
    "\n",
    "limit = {'$limit': 10000}\n",
    "\n",
    "pipeline = [matchNE, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('NE')\n",
    "\n",
    "pipeline = [matchSE, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('SE')\n",
    "\n",
    "pipeline = [matchNW, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('NW')\n",
    "\n",
    "pipeline = [matchSW, limit]\n",
    "\n",
    "for tweet in db.tweet.aggregate(pipeline):\n",
    "    sentence_list.append(tweet['text'])\n",
    "    location_list.append('SW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add your own sentence here\n",
    "your_sentence = \"It's snowing in New York!\"\n",
    "sentence_list.append(your_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Tweet_Tokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = TweetTokenizer()\n",
    "    def __call__(self, doc):\n",
    "        return self.wnl.tokenize(doc)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import dok_matrix, hstack\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np    \n",
    "\n",
    "def make_features(corpus):\n",
    "#     vectorizer = CountVectorizer(tokenizer=Tweet_Tokenizer(), analyzer='word', min_df=0)\n",
    "#     return vectorizer.fit_transform(corpus), vectorizer.get_feature_names()\n",
    "\n",
    "    # Basic BOW\n",
    "    vectorizer = CountVectorizer(tokenizer=Tweet_Tokenizer(), analyzer='word', min_df=2)\n",
    "    X_BOW = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Additional Features (need to add more)\n",
    "    NUM_OF_FEATS = 1\n",
    "    X_ADD = dok_matrix((len(corpus), NUM_OF_FEATS))\n",
    "    tt = Tweet_Tokenizer()\n",
    "\n",
    "    # Find Length Percentage\n",
    "    num_token_list = np.array([len(tt.__call__(text)) for text in corpus])\n",
    "    num_token_list = np.argsort(num_token_list)\n",
    "    length_percentage_dict = {num_token_list[i]: i * 1.0 / len(num_token_list) for i in range(len(num_token_list))}\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        X_ADD[i, 0] = length_percentage_dict[i]\n",
    "\n",
    "    # Concatenate\n",
    "    X = hstack([X_BOW, X_ADD])\n",
    "\n",
    "    return X, vectorizer.get_feature_names() + ['<length>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'coo_matrix' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c7a3ce9d9347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence_vector_list, vector_name_list = make_features(sentence_list)\\nyour_sentence_vector = sentence_vector_list[-1]\\nsentence_vector_list = sentence_vector_list[:-1]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object does not support indexing"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence_vector_list, vector_name_list = make_features(sentence_list)\n",
    "your_sentence_vector = sentence_vector_list[-1]\n",
    "sentence_vector_list = sentence_vector_list[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_vectors, test_vectors, training_locations, test_locations =\\\n",
    "    train_test_split(sentence_vector_list, location_list, test_size=0.1, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.6 s, sys: 75.9 ms, total: 41.7 s\n",
      "Wall time: 41.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=5000, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_clf.fit(training_vectors, training_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.76 ms, sys: 734 µs, total: 2.5 ms\n",
      "Wall time: 1.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_test_locations = lr_clf.predict(test_vectors)\n",
    "predicted_your_sentence_location = lr_clf.predict(your_sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.4035\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set accuracy: \"\\\n",
    "      + str(accuracy_score(test_locations, predicted_test_locations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of your sentence: ['NW']\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of your sentence: \" + str(predicted_your_sentence_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Find Top Weights in Logistic Regression Classifier\n",
    "theta_NE = lr_clf.coef_[0]\n",
    "theta_NW = lr_clf.coef_[1]\n",
    "theta_SE = lr_clf.coef_[2]\n",
    "theta_SW = lr_clf.coef_[3]\n",
    "weights_NE = dict()\n",
    "weights_NW = dict()\n",
    "weights_SE = dict()\n",
    "weights_SW = dict()\n",
    "\n",
    "for feature, weight in zip(vector_name_list, theta_NE):\n",
    "    weights_NE[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_NW):\n",
    "    weights_NW[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_SE):\n",
    "    weights_SE[feature] = weight\n",
    "for feature, weight in zip(vector_name_list, theta_SW):\n",
    "    weights_SW[feature] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Scoring Logistic Regression Weights\n",
      "\n",
      "ny\t\tNE\t\t1.71115443298\n",
      "#nyc\t\tNE\t\t1.63361423885\n",
      "#specialreport\t\tNE\t\t1.61197706748\n",
      "toronto\t\tNE\t\t1.59365915398\n",
      "ebay\t\tNE\t\t1.55748699816\n",
      "ohio\t\tNE\t\t1.52965782532\n",
      "@bottomoso\t\tNE\t\t1.51828566167\n",
      "@steff__r\t\tNE\t\t1.50676301868\n",
      "detroit\t\tNE\t\t1.48532423636\n",
      "@nicoleee1_\t\tNE\t\t1.46714299836\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "@cloacamaxima01\t\tNW\t\t2.22805003937\n",
      "portland\t\tNW\t\t1.95345939666\n",
      "@russia\t\tNW\t\t1.84209398054\n",
      "seattle\t\tNW\t\t1.8303613595\n",
      "bay\t\tNW\t\t1.72858493205\n",
      "colorado\t\tNW\t\t1.64933731643\n",
      "francisco\t\tNW\t\t1.61359340601\n",
      "#job\t\tNW\t\t1.60769815923\n",
      "wa\t\tNW\t\t1.59610434078\n",
      "utah\t\tNW\t\t1.53873501498\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "@gibson326\t\tSE\t\t2.09738893804\n",
      "fl\t\tSE\t\t2.08730570408\n",
      "alabama\t\tSE\t\t2.01769496561\n",
      "@taylorswift13\t\tSE\t\t1.91531519041\n",
      "@negrosubversive\t\tSE\t\t1.82964847525\n",
      "@mycfe\t\tSE\t\t1.70256018502\n",
      "tx\t\tSE\t\t1.59764313345\n",
      "orleans\t\tSE\t\t1.57303915224\n",
      "#ghc16\t\tSE\t\t1.55480352796\n",
      "butter\t\tSE\t\t1.41978836958\n",
      "\n",
      "\n",
      "################################\n",
      "\n",
      "\n",
      "az\t\tSW\t\t2.14579573694\n",
      "ca\t\tSW\t\t2.08916253271\n",
      "@im_beyondgreat\t\tSW\t\t1.96024525811\n",
      "disneyland\t\tSW\t\t1.81969232282\n",
      "#losangeles\t\tSW\t\t1.76956889366\n",
      "hollywood\t\tSW\t\t1.71354147983\n",
      "steroline\t\tSW\t\t1.69610972466\n",
      "arizona\t\tSW\t\t1.69033595728\n",
      "@oxthebeardog\t\tSW\t\t1.66660365118\n",
      "@evan_b\t\tSW\t\t1.64359908892\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Scoring Logistic Regression Weights\\n\")\n",
    "top_features_NE = sorted(weights_NE, key=lambda x:weights_NE[x], reverse=True)[:10]\n",
    "for word in top_features_NE:\n",
    "    print(str(word) + '\\t\\tNE\\t\\t' + str(weights_NE[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_NW = sorted(weights_NW, key=lambda x:weights_NW[x], reverse=True)[:10]\n",
    "for word in top_features_NW:\n",
    "    print(str(word) + '\\t\\tNW\\t\\t' + str(weights_NW[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_SE = sorted(weights_SE, key=lambda x:weights_SE[x], reverse=True)[:10]\n",
    "for word in top_features_SE:\n",
    "    print(str(word) + '\\t\\tSE\\t\\t' + str(weights_SE[word]))\n",
    "\n",
    "print('\\n')\n",
    "print('################################')\n",
    "print('\\n')\n",
    "\n",
    "top_features_SW = sorted(weights_SW, key=lambda x:weights_SW[x], reverse=True)[:10]\n",
    "for word in top_features_SW:\n",
    "    print(str(word) + '\\t\\tSW\\t\\t' + str(weights_SW[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Find more in Chapter 1 and 2 of the book below\n",
    "https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
